

# planet/scripts/train.py
def start(logdir, args):
def resume(logdir, args):
def process(logdir, config, args):

def main(args):
    experiment = training.Experiment(
        process_fn=functools.partial(process, args=args),
        start_fn=functools.partial(start, args=args),
        resume_fn=functools.partial(resume, args=args), ...)

    for run in experiment:          # the method __iter__(self) defined in experiment.
        for unused_score in run:    # the method __iter__(self) defined in run.
            pass

class Experiment(object):
    def __iter__(self):

num_runs

# planet/training/running.py
class Run(object):
    def __iter__(self):
        """Iterate over the process function and finalize the log directory."""
        args = self._init_fn and self._init_fn(self._logdir)


        # planet/scripts/train.py
        def start(logdir, args):
            config = getattr(configs, args.config)(config, args.params)  # configs.(args.config) = configs.default, task is defined.
            training.utility.collect_initial_episodes(config)

            # planet/training/utility.py
            def collect_initial_episodes(config):
                control.random_episodes(...)

                # planet/control/random_episodes.py
                def random_episodes(env_ctor, num_episodes, output_dir=None):
                    env = env_ctor()
                    env = wrappers.CollectGymDataset(env, output_dir)
                    obs = env.reset()
                    while not done:
                        obs, _, done, info = env.step(action)


        for value in self._process_fn(self._logdir, *args):
            if not self._running[0]:
                break
            yield value


        # planet/scripts/train.py
        def process(logdir, config, args):
            dataset = tools.numpy_episodes(...)
            for score in training.utility.train(training.define_model, dataset, logdir, config):
                yield score

            # planet/training/utility.py
            def train(model_fn, datasets, logdir, config):
                trainer = trainer_.Trainer(logdir, config=config)        #  trainer_ is trainer.py
                data = get_batch(datasets, trainer.phase, trainer.reset)
                score, summary = model_fn(data, trainer, config)         #  model_fn is training.define_model.
                    # planet/training/define_model.py
                    def define_model(data, trainer, config):
                        ...
                        collect_summary, _ = tf.cond(should_collect, functools.partial(utility.simulate_episodes, config, params, graph, name), ...)
                            # planet/training/utility.py
                            def simulate_episodes(config, params, graph, name):
                                def env_ctor():
                                    env = params.task.env_ctor()
                                    if params.save_episode_dir:
                                    env = control.wrappers.CollectGymDataset(env, params.save_episode_dir)
                                    env = control.wrappers.ConcatObservation(env, ['image'])
                                    return env
                                for index in range(params.batch_size):   # params.batch_size = 1  #####################
                                    summary, return_ = control.simulate(graph.step, env_ctor, params.task.max_length, 1, agent_config, name=name)
                                        # I. planet/control/simulate.py
                                        def simulate(step, env_ctor, duration, num_agents, agent_config, env_processes=False, name='simulate'):
                                            return_, image, action, reward = collect_rollouts(step=step, env_ctor=env_ctor, duration=duration, num_agents=num_agents, agent_config=agent_config, env_processes=env_processes )
                                            # II.
                                            def collect_rollouts(step, env_ctor, duration, num_agents, agent_config, env_processes):
                                                batch_env = define_batch_env(env_ctor, num_agents, env_processes)
                                                    # IIa.
                                                    def define_batch_env(env_ctor, num_agents, env_processes):
                                                        env = batch_env.BatchEnv(envs, blocking=not env_processes)
                                                        env = in_graph_batch_env.InGraphBatchEnv(env)
                                                        return env
                                                agent = mpc_agent.MPCAgent(batch_env, step, False, False, agent_config)
                                                def simulate_fn(unused_last, step):
                                                    done, score, unused_summary = simulate_step( batch_env, agent, log=False, reset=tf.equal(step, 0))
                                                done, score, image, action, reward = tf.scan( simulate_fn, tf.range(duration), initializer, parallel_iterations=1)
                                                # III.
                                                def simulate_step()
                                                    ...
                                                    _define_step()
                                                    # IV.
                                                    def _define_step():
                                                        action, step_summary = algo.perform(agent_indices, prevob)    # get action from the planner.(algo = agent.)
                                                        with tf.control_dependencies([batch_env.step(action)]):       # interact with the env.
                                                            ...
                                                            # planet_A/planet/control/mpc_agent.py
                                                            class MPCAgent(object):
                                                                ...
                                                                def perform(self, agent_indices, observ):
                                                                    action = self._config.planner(self._cell, self._config.objective, state,embedded.shape[1:].as_list(),prev_action.shape[1:].as_list())
                                                                        # planet_A/planet/scripts/configs.py
                                                                        def _define_simulation():
                                                                            planner = functools.partial(control.planning.cross_entropy_method, ...)




                trainer.add_phase(...)
                trainer.add_phase(...)
                for score in trainer.iterate(config.max_steps):          # training...
                    yield score

                # planet/training/trainer.py
                class Trainer(object):
                    def iterate(self, max_step=None, sess=None):
                        while True:                                      # TRAINING LOOP
                            global_step = sess.run(self._global_step)
                            if max_step and global_step >= max_step:
                                break
                            summary, mean_score, global_step = sess.run(phase.op, phase.feed)
                            if self._is_every_steps(phase_step, phase.batch_size, phase.report_every):
                                tf.logging.info('Score {}.'.format(mean_score))
                                yield mean_score




env.py
ENV_CONFIG = {
    ...
    "x_res": 128, #64,  # cv2.resize()
    "y_res": 128, #64,  # cv2.resize()
    ...
    }

tasks.py
def _dm_control_env_carla(action_repeat, max_length, env_name):
(128, 128)

conv_ha.py
assert mean.shape[1:].as_list() == [128, 128, 3], mean.shape


# planet/control/simulate.py
def collect_rollouts():
    ...
    def simulate_fn():
        ...
    done, score, image, action, reward = tf.scan( simulate_fn, tf.range(duration),initializer, parallel_iterations=1)



# planet/scripts/tasks.py
def _dm_control_env_carla(action_repeat, max_length, env_name, img_size):
    env = control.wrappers.ExternalProcess(env_ctor)

    # planet/control/wrappers.py
    class ExternalProcess(object):
        def __init__(self, constructor):
            self._process = multiprocessing.Process(target=self._worker, args=(constructor, conn))
            self._process.start()

            # planet/control/wrappers.py
            def _worker(self, constructor, conn):
                result = getattr(env, name)(*args, **kwargs)

                # name = 'reset'
                # planet/control/wrappers.py
                class ConvertTo32Bit(object):
                    def reset(self):
                        observ = self._env.reset()

                # name = 'observation_sapce'
                # planet/control/wrappers.py
                class ConvertTo32Bit(object):
                    def __getattr__(self, name):
                        return getattr(self._env, name)

                        # planet/control/wrappers.py
                        class PixelObservations(object):
                            @property
                            high = {np.uint8: 255, np.float: 1.0}[self._dtype]
                            image = gym.spaces.Box(0, high, self._size + (3,), dtype=self._dtype)
                            ...


改动：
config.train_steps = int(params.get('train_steps', 500))  # train_steps for each epoch
sim.steps_after = params.get('collect_every', 50)    # sim after 5000 steps
sim.steps_every = params.get('collect_every', 50)    # sim every 5000 steps

_read_episodes_scan（）函数
'step_error'异常处理

###
把buffersize改成50，结合_read_episodes_scan（max_episodes=100）函数看训练效果以及cpu内存占用.对比原来的_read_episodes_scan函数和buffersize。